{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mtrand.RandomState at 0x1ddc09c72d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reading in the data that was prepared for modeling. In the notebook Feature Engineering and Pre-processing, we ensured that our X variable and test data had the same features so that any trained model would be able to predict the probability of West Nile virus in our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../../Data/X.csv',index_col=[0])\n",
    "y = pd.read_csv('../../Data/y.csv',index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before modeling, it is important to train test split. This is done so that we can later test to make sure that our model is able to predict accurately on unseen data. In this specific case, we are training our model on 70 percent of our data, and then testing it on the remaining 30 percent. By stratifying on our y variable, we are saying that we want the proportion of cases where West Nile virus is present to be the same in both the testing and training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y[\"WnvPresent\"], test_size=0.3, random_state=42,stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Pipelines are a way to streamline a lot of the routine processes by encapsulating little pieces of logic into one function call, making it easier to model our data. They are set up with the fit/transform/predict functionality so that you could fit a whole pipeline to the training data while also transforming the test data without having to do each step individually. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’. In the model below, we set Standard Scaler and Adaptive Boost as its 'steps' parameters in order to tell our pipeline which estimators we want to fit. We can see that we fit them both while also finding optimal parameters all in one step. \n",
    ">>Step 1: Standardization of datasets is an important requirement for many machine learning estimators. Datasets that are not standardized will cause problems for models because they do not more or less look like standard normally distributed data (Gaussian with zero mean and unit variance). We can use Standard Scaler, a preprocessing module, to standardize features by removing the mean and scaling to unit variance. It scales data by dividing non-constant features by their standard deviation. This is the most used module due to the fact that it computes the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. By using Standard Scaler, we are allowing each feature to be considered on the same scale as one another. This makes our computations more efficient.\n",
    "\n",
    ">> Step 2: Adaptive Boosting is meant to enhance machine learning algorithms. The default for this model uses the Decision Tree Classifier, but it can also be used to enhance other models. This model performs the best on binary classification, and it works by creating a decision tree which it learns from each decision it makes. How it does this is by fitting a single decision tree for our data, and looks at the predictions that it got wrong. It then changes the data so that the observations that were incorrect have more weight for the next decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('SS', StandardScaler()),\n",
    "    ('ADA', AdaBoostClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced')))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This cell specifies the specific hyper Parameters that we want to test to see which combination of hyper parameters perform the best. These hyper parameters are:\n",
    ">> n_estimators - # of Estimators: This parameter refers to how many Models should be fit and learned upon \n",
    "\n",
    ">> learning_rate : The learning rate is the rate of how much each model contributes to the weights each decision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "              'ADA__n_estimators':[30,50,70,100,150],\n",
    "              'ADA__learning_rate': [.05,.1,.5,.7,1.0]\n",
    "              \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Grid Search is a module that performs parameter tuning. Parameter tuning is the process of selecting the values for a model’s parameters that maximize the accuracy of the model. Grid Search does this by exhaustively generating candidates from a grid of parameter values specified with the `param_grid` parameter. When “fitting” Grid Search on a dataset, all the possible combinations of parameter values are evaluated, and the best combination is chosen. In our case, we peform Grid Search in order to find the parameters that give us the highest score.\n",
    ">>Instead of scoring the model with accuracy, we scored the model with `roc_auc` , which stands for receiver operating characteristic area under the curve. This is used to evaluate the performance of a binary classification system, and gives us insight into how our model is doing. It does this because our classes are unbalanced. We could predict zero for every data point, and could have a 95% accuracy score because 95% of the data is in our negative class. Using `roc_auc` accounts for the true positives and true negatives that we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('SS', StandardScaler(copy=True, with_mean=True, with_std=True)), ('ADA', AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impu...ne,\n",
       "            splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'ADA__n_estimators': [30, 50, 70, 100, 150], 'ADA__learning_rate': [0.05, 0.1, 0.5, 0.7, 1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaBoost = GridSearchCV(pipe, param_grid= param_grid, scoring = 'roc_auc' )\n",
    "AdaBoost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values for ADA__learning_rate That gave us the highest roc_auc score is 0.7\n",
      "The values for ADA__n_estimators That gave us the highest roc_auc score is 50\n"
     ]
    }
   ],
   "source": [
    "for param in AdaBoost.best_params_:\n",
    "    print('The values for',param,'That gave us the highest roc_auc score is',AdaBoost.best_params_[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training score of the model is 0.9958461107178337\n",
      "The testing score of the model is 0.8010500045652373\n"
     ]
    }
   ],
   "source": [
    "print('The training score of the model is',AdaBoost.score(X_train,y_train))\n",
    "print('The testing score of the model is',AdaBoost.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../Assets/AdaBoost.pkl', 'wb+') as f:\n",
    "    pickle.dump(AdaBoost, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
