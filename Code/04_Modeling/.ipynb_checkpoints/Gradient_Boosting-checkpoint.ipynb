{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mtrand.RandomState at 0x2370b968288>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reading in the data that was prepared for modeling. In the notebook Feature Engineering and Pre-processing, we ensured that our X variable and test data had the same features so that any trained model would be able to predict the probability of West Nile virus in our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../../Data/X.csv',index_col=[0])\n",
    "y = pd.read_csv('../../Data/y.csv',index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before modeling, it is important to train test split. This is done so that we can later test to make sure that our model is able to predict accurately on unseen data. In this specific case, we are training our model on 70 percent of our data, and then testing it on the remaining 30 percent. By stratifying on our y variable, we are saying that we want the proportion of cases where West Nile virus is present to be the same in both the testing and training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Pipelines are a way to streamline a lot of the routine processes by encapsulating little pieces of logic into one function call, making it easier to model our data. They are set up with the fit/transform/predict functionality so that you could fit a whole pipeline to the training data while also transforming the test data without having to do each step individually. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’. In the model below, we set Standard Scaler and Gradient Boosting as its 'steps' parameters in order to tell our pipeline which estimators we want to fit. We can see that we fit them both while also finding optimal parameters all in one step. \n",
    ">>Step 1: Standardization of datasets is an important requirement for many machine learning estimators. Datasets that are not standardized will cause problems for models because they do not more or less look like standard normally distributed data (Gaussian with zero mean and unit variance). We can use Standard Scaler, a preprocessing module, to standardize features by removing the mean and scaling to unit variance. It scales data by dividing non-constant features by their standard deviation. This is the most used module due to the fact that it computes the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. By using Standard Scaler, we are allowing each feature to be considered on the same scale as one another. This makes our computations more efficient.\n",
    "\n",
    ">> Step 2: Gradient Boosting is a classification model that uses \"weak\" models, and uses the residuals to then build a more robust model. It first creates a single decision tree, which is likely to be overfit to the training data, and makes predictions. The model then looks at how wrong it was, then fits another decision tree based on the residuals over and over again, combining what each tree has learned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "       ('ss',StandardScaler()),\n",
    "       ('GBC',GradientBoostingClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This cell specifies the specific hyper parameters that we want to test to see which combination of hyper parameters perform the best. These hyper parameters are:\n",
    ">> n_estimators: This parameter determines the number of decision trees that are created when fitting our \n",
    "    boosting model\n",
    "\n",
    ">> max depth: This determines the number of splits that each individual decision tree will make before \n",
    "    predicting and creating a new tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'GBC__n_estimators':[100,150,200],\n",
    "    'GBC__min_samples_split': [2, 3],\n",
    "    'GBC__min_samples_leaf': [3, 5],\n",
    "    'GBC__max_depth':[3,5,7]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Grid Search is a module that performs parameter tuning. Parameter tuning is the process of selecting the values for a model’s parameters that maximize the accuracy of the model. Grid Search does this by exhaustively generating candidates from a grid of parameter values specified with the `param_grid` parameter. When “fitting” Grid Search on a dataset, all the possible combinations of parameter values are evaluated, and the best combination is chosen. In our case, we peform Grid Search in order to find the parameters that give us the highest score.\n",
    ">>Instead of scoring the model with accuracy, we scored the model with `roc_auc` , which stands for receiver operating characteristic area under the curve. This is used to evaluate the performance of a binary classification system, and gives us insight into how our model is doing. It does this because our classes are unbalanced. We could predict zero for every data point, and could have a 95% accuracy score because 95% of the data is in our negative class. Using `roc_auc` accounts for the true positives and true negatives that we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('ss', StandardScaler(copy=True, with_mean=True, with_std=True)), ('GBC', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0....      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'GBC__n_estimators': [100, 150, 200], 'GBC__min_samples_split': [2, 3], 'GBC__min_samples_leaf': [3, 5], 'GBC__max_depth': [3, 5, 7]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradBoost = GridSearchCV(pipe,param_grid=params,scoring='roc_auc')\n",
    "GradBoost.fit(X_train,y_train['WnvPresent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value for GBC__max_depth that had the highest roc_auc score is 3\n",
      "The value for GBC__min_samples_leaf that had the highest roc_auc score is 5\n",
      "The value for GBC__min_samples_split that had the highest roc_auc score is 2\n",
      "The value for GBC__n_estimators that had the highest roc_auc score is 100\n"
     ]
    }
   ],
   "source": [
    "for params in GradBoost.best_params_:\n",
    "    print('The value for',params,'that had the highest roc_auc score is',GradBoost.best_params_[params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training score of the model is 0.8853349955087059\n",
      "The testing score of the model is 0.87556380679916\n"
     ]
    }
   ],
   "source": [
    "print('The training score of the model is',GradBoost.score(X_train,y_train))\n",
    "print('The testing score of the model is',GradBoost.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../Assets/GradBoost.pkl','wb+') as f:\n",
    "    pickle.dump(GradBoost,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
